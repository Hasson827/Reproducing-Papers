{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG论文复现\n",
    "\n",
    "## ⭐ VGG的主要特点\n",
    "1. **结构简单：**\n",
    "\n",
    "全部使用3×3卷积核，步幅为1，填充为1，保持特征图大小不变。\n",
    "\n",
    "2×2最大池化层，步幅为2，用于下采样。\n",
    "\n",
    "2. **深层网络：**\n",
    "\n",
    "通过堆叠多个卷积层来增加深度，从而提升模型的表达能力。\n",
    "\n",
    "例如，VGG16 的架构为：\n",
    "```css\n",
    "[Conv3-64] ×2 → MaxPool → [Conv3-128] ×2 → MaxPool → \n",
    "[Conv3-256] ×3 → MaxPool → [Conv3-512] ×3 → MaxPool → \n",
    "[Conv3-512] ×3 → MaxPool → FC-4096 → FC-4096 → FC-1000 → Softmax\n",
    "```\n",
    "3. **全连接层：**\n",
    "\n",
    "在卷积层之后，VGG使用全连接层（FC层）进行分类。\n",
    "\n",
    "VGG16和VGG19都使用两个4096维的全连接层和一个1000维的输出层（对应ImageNet 1000类）。\n",
    "\n",
    "4. **权重可迁移：**\n",
    "\n",
    "VGG模型在ImageNet上的预训练权重常用于迁移学习，在许多计算机视觉任务（如目标检测、语义分割）中表现优异。\n",
    "\n",
    "## ⚡ VGG的优势与劣势\n",
    "1. **✅ 优点：**\n",
    "\n",
    "架构统一且易于理解。\n",
    "\n",
    "性能优异，尤其在ImageNet图像分类挑战赛中取得了优异成绩。\n",
    "\n",
    "2.**❌ 缺点：**\n",
    "\n",
    "参数量巨大（VGG16约有138M参数），导致存储和推理成本高。\n",
    "\n",
    "计算复杂度高，不适用于资源受限的设备。\n",
    "\n",
    "如果你在研究扩散模型，VGG有时也用作特征提取器来计算感知损失（perceptual loss），因为它在捕捉图像语义特征方面表现很好。\n",
    "\n",
    "## ⚙️ VGG的架构\n",
    "<img src=\"img/VGG架构.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG的代码实现\n",
    "由于VGG的网络结构比较复杂，我们通过分析VGG结构后会发现它实际上是由许多类似的“VGG块”构成，每一个“VGG块”的实现则相对比较简单。最后使用nn.Sequential()函数将这些“VGG块”串联起来，就可以得到完整的VGG网络。\n",
    "\n",
    "下面我们来实现VGG网络的构建。此处的数据库是ImageNet,我们同样采用tiny-imagenet数据集。（同AlexNet）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 必要库的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取当前文件的父目录\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname('__file__'),'..'))\n",
    "# 如果父目录不在 sys.path中，则添加\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "import utils\n",
    "    \n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                      else 'mps' if torch.mps.is_available()\n",
    "                      else 'cpu')\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.导入数据库并转换为DataLoader格式\n",
    "\n",
    "在这一部分中，我们使用torchvision.transforms 仿照原论文对图像进行预处理：\n",
    "- `Resize` 和 `CenterCrop`：调整图像大小以匹配模型输入。\n",
    "- `ToTensor`：将图像转换为张量。\n",
    "- `Normalize`：对像素值进行归一化，有助于模型更快收敛。\n",
    "- 在训练集中用 `RandomHorizontalFlip`使得图像随机水平翻转，增强图像\n",
    "\n",
    "⚠️ **初学者提示**：\n",
    "- 确保 `mean` 和 `std` 值与预训练模型使用的值一致。\n",
    "- 输入图像的大小应与模型期望的输入大小匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG原论文的预处理流程\n",
    "transform = {\n",
    "    'train':transforms.Compose([\n",
    "    transforms.Resize(256),                      # 缩放最短边为256像素\n",
    "    transforms.RandomCrop(224),                  # 中心裁剪224x224\n",
    "    transforms.RandomHorizontalFlip(p=0.5),      # 以50%概率水平翻转\n",
    "    transforms.ToTensor(),                       # 转换为Tensor并缩放到[0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 按ImageNet均值和标准差归一化\n",
    "                         std=[0.229, 0.224, 0.225])]),\n",
    "    'val':transforms.Compose([\n",
    "    transforms.Resize(256),                      # 缩放最短边为256像素\n",
    "    transforms.CenterCrop(224),                  # 中心裁剪224x224\n",
    "    transforms.ToTensor(),                       # 转换为Tensor并缩放到[0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 按ImageNet均值和标准差归一化\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "    ])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n03444034\n"
     ]
    }
   ],
   "source": [
    "# 读取验证集的标签\n",
    "val_labels = {}\n",
    "with open('../Datasets/tiny-imagenet-200/val/val_annotations.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        filename, label = parts[0], parts[1]\n",
    "        val_labels[filename] = label\n",
    "print(val_labels['val_0.JPEG'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先将训练数据集加载到DataLoader中\n",
    "ImageNet_train = datasets.ImageFolder(root='../Datasets/tiny-imagenet-200/train',transform=transform['train'])\n",
    "ImageNet_train_iter = DataLoader(ImageNet_train,batch_size=64,shuffle=True,num_workers=0)\n",
    "\n",
    "# 自定义验证集数据集\n",
    "class TinyImageNetValDataset(Dataset):\n",
    "    def __init__(self, img_dir, val_labels, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_dict = val_labels\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(list(set(self.img_dict.values())))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.img_list = list(self.img_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_list[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.class_to_idx[self.img_dict[img_name]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "ImageNet_val = TinyImageNetValDataset(\n",
    "    img_dir='../Datasets/tiny-imagenet-200/val/images',\n",
    "    val_labels=val_labels,\n",
    "    transform=transform['val'])\n",
    "ImageNet_val_iter = DataLoader(ImageNet_val, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n",
      "tensor([178, 141, 140, 109, 119,  64,  18, 112,  73, 137, 187,  53, 117, 161,\n",
      "         13,  21, 155,  27, 160, 195,  89,  51, 177,  16, 133, 122,  45,  49,\n",
      "        183, 115, 152,  22, 160,  28, 183, 103,  63, 176, 187, 165,  75, 110,\n",
      "         87,  50, 161, 145,  45, 115, 186,  14, 174, 101,  63,  96, 189, 172,\n",
      "        144,  39,  25, 139, 199,  23,  29, 121])\n",
      "2\n",
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n",
      "tensor([107, 139, 140,  69,  69, 161, 147,  73, 145,  39, 158, 188,  39, 120,\n",
      "         21,  79,  33, 165, 153, 119, 123, 124,  18,   1, 170, 141, 171, 158,\n",
      "         79,  25, 121, 179, 124, 130, 176,  86,   6, 129, 142, 127,  33,  33,\n",
      "         44,  40, 110,  40, 159, 184,  57,  80,   9,  34,  39,  14,  75, 199,\n",
      "         10,  99,  66, 160,  69, 177,  25, 101])\n"
     ]
    }
   ],
   "source": [
    "# 尝试输出两个DataLoader中的第一批张量的形状\n",
    "img_train = next(iter(ImageNet_train_iter))\n",
    "print(len(img_train))\n",
    "print(img_train[0].shape)\n",
    "print(img_train[1].shape)\n",
    "print(img_train[1])\n",
    "\n",
    "img_val = next(iter(ImageNet_val_iter))\n",
    "print(len(img_val))\n",
    "print(img_val[0].shape)\n",
    "print(img_val[1].shape)\n",
    "print(img_val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 定义VGG块\n",
    "#### 🔎 VGG块的构建规律\n",
    "\n",
    "1. **卷积层：** 每个VGG块包含2到3个连续的卷积层，每个卷积层使用：\n",
    "- 卷积核大小：3×3\n",
    "- 步幅（stride）：1\n",
    "- 填充（padding）：1（保持特征图大小不变）\n",
    "\n",
    "2. **激活函数：** 每个卷积层后面跟一个ReLU激活。\n",
    "\n",
    "3. **通道数：** 随着网络深度的增加，每个块的输出通道数量依次为：\n",
    "```css\n",
    "64 → 128 → 256 → 512 → 512\n",
    "```\n",
    "4. **池化层：** 每个VGG块后面接一个2×2最大池化层，步幅为2，用于特征下采样。\n",
    "\n",
    "#### 🏗️ VGG不同架构的VGG块配置\n",
    "| 网络结构   | Block1        | Block2        | Block3          | Block4          | Block5          |\n",
    "|------------|----------------|----------------|------------------|------------------|------------------|\n",
    "| **VGG11**  | 1×Conv64       | 1×Conv128      | 2×Conv256        | 2×Conv512        | 2×Conv512        |\n",
    "| **VGG13**  | 2×Conv64       | 2×Conv128      | 2×Conv256        | 2×Conv512        | 2×Conv512        |\n",
    "| **VGG16**  | 2×Conv64       | 2×Conv128      | 3×Conv256        | 3×Conv512        | 3×Conv512        |\n",
    "| **VGG19**  | 2×Conv64       | 2×Conv128      | 4×Conv256        | 4×Conv512        | 4×Conv512        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(num_convs,in_channels,out_channels):\n",
    "    \"\"\"\n",
    "    构建VGG块:多个3*3卷积层 + ReLU激活 + 2*2最大池化。\n",
    "    \n",
    "    Args:\n",
    "        num_convs (int): 卷积层数量\n",
    "        in_channels (int): 输入通道数\n",
    "        out_channels (int): 输出通道数\n",
    "    \n",
    "    Returns:\n",
    "        nn.Sequential: VGG块\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1,padding=1))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        in_channels = out_channels\n",
    "    layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 搭建VGG神经网络模型\n",
    "实际上我们完全可以定义一个比较通用的VGG模型，通过不同的参数输入可以构建不同深度和广度的VGG神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (4): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace=True)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Dropout(p=0.5, inplace=False)\n",
      "    (7): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self,conv_arch,fc_features=4096,fc_hidden_units=4096,num_classes=1000):\n",
    "        \"\"\"\n",
    "        构建完整的VGG网络。\n",
    "        \n",
    "        Args:\n",
    "            conv_arch (list): VGG卷积部分的配置,每个元素为 (卷积层数, 输入通道, 输出通道)\n",
    "            fc_features (int): 第一个全连接层的输入特征数\n",
    "            fc_hidden_units (int): 隐藏层的神经元数\n",
    "            num_classes (int): 分类类别数\n",
    "        \"\"\"\n",
    "        super(VGG,self).__init__()\n",
    "        \n",
    "        # 计算全连接层输入的特征数 (根据卷积层输出的尺寸)\n",
    "        self.fc_features = fc_features  # 自定义输入特征数量，可以根据卷积输出尺寸来计算\n",
    "        self.fc_hidden_units = fc_hidden_units\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.features = self.make_layers(conv_arch)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(fc_features,fc_hidden_units),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(fc_hidden_units, fc_hidden_units),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(fc_hidden_units, num_classes)\n",
    "        )\n",
    "\n",
    "    def make_layers(self,conv_arch):\n",
    "        layers=[]\n",
    "        for (num_convs,in_channels,out_channels) in conv_arch:\n",
    "            layers.append(vgg_block(num_convs,in_channels,out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "net = VGG([(1, 3, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512)]).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📚 VGG不同架构的配置 (命名的来源是卷积层数+全连接层数)\n",
    "vgg11_arch = [(1, 3, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512)]\n",
    "vgg13_arch = [(2, 3, 64), (2, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512)]\n",
    "vgg16_arch = [(2, 3, 64), (2, 64, 128), (3, 128, 256), (3, 256, 512), (3, 512, 512)]\n",
    "vgg19_arch = [(2, 3, 64), (2, 64, 128), (4, 128, 256), (4, 256, 512), (4, 512, 512)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG11 输出形状: torch.Size([1, 1000])\n",
      "VGG13 输出形状: torch.Size([1, 1000])\n",
      "VGG16 输出形状: torch.Size([1, 1000])\n",
      "VGG19 输出形状: torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "# 🏃‍♂️ **测试不同VGG模型**\n",
    "def test_vgg(arch, name):\n",
    "    model = VGG(arch,fc_features=512*7*7)\n",
    "    X = torch.randn(1, 3, 224, 224)  # 模拟输入\n",
    "    output = model(X)\n",
    "    print(f\"{name} 输出形状: {output.shape}\")  # 期望输出: torch.Size([1, 1000])\n",
    "\n",
    "\n",
    "\n",
    "test_vgg(vgg11_arch, \"VGG11\")\n",
    "test_vgg(vgg13_arch, \"VGG13\")\n",
    "test_vgg(vgg16_arch, \"VGG16\")\n",
    "test_vgg(vgg19_arch, \"VGG19\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 用VGG11来训练！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnet = VGG(vgg11_arch,fc_features=512*7*7,num_classes=200).to(device)\\nloss_fn = nn.CrossEntropyLoss()\\noptimizer = optim.Adam(params=net.parameters(),lr=0.001)\\nutils.training_loop(net,loss_fn=loss_fn,optimizer=optimizer,train_iter=ImageNet_train_iter,test_iter=ImageNet_val_iter,epochs=15,device=device)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "net = VGG(vgg11_arch,fc_features=512*7*7,num_classes=200).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(),lr=0.001)\n",
    "utils.training_loop(net,loss_fn=loss_fn,optimizer=optimizer,train_iter=ImageNet_train_iter,test_iter=ImageNet_val_iter,epochs=15,device=device)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
