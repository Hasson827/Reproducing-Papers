{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogLeNet论文复现\n",
    "\n",
    "## **GoogLeNet的主要特点**\n",
    "\n",
    "GoogLeNet是由Google提出的一种深度卷积神经网络架构，它的主要特点包括：\n",
    "\n",
    "1. **Inception模块**：\n",
    "   - GoogLeNet的核心创新是引入了Inception模块。该模块通过在同一层上并行执行多种卷积操作（如不同大小的卷积核）和池化操作，以捕获不同尺度的特征。\n",
    "   - 每个Inception模块包含多个卷积层（不同卷积核尺寸）和池化层（最大池化与平均池化），并通过1x1卷积进行特征压缩和维度匹配。\n",
    "\n",
    "2. **1x1卷积**：\n",
    "   - Inception模块中使用了大量的1x1卷积层，它可以用来减少计算量和参数数量。\n",
    "   - 1x1卷积被用于降低维度和进行特征融合，使得模型在保持高效性的同时，能够捕获更丰富的特征。\n",
    "\n",
    "3. **深度可分离卷积（Depthwise Separable Convolutions）**：\n",
    "   - 在网络的设计中，GoogLeNet使用了深度可分离卷积，减少了卷积操作的计算量，并提高了模型的效率。\n",
    "\n",
    "4. **全局平均池化**：\n",
    "   - GoogLeNet采用了全局平均池化（Global Average Pooling）来代替传统的全连接层，从而减少了模型的参数数量，降低了过拟合的风险。\n",
    "\n",
    "5. **更深的网络结构**：\n",
    "   - 相比于AlexNet和VGG，GoogLeNet具有更深的网络架构，但通过高效的Inception模块，保持了计算效率。\n",
    "\n",
    "---\n",
    "\n",
    "## **GoogLeNet的优势**\n",
    "\n",
    "1. **高效性**：\n",
    "   - GoogLeNet通过使用Inception模块以及1x1卷积，大大减少了计算量和参数数量，从而在处理大规模数据时提高了效率。\n",
    "\n",
    "2. **较少的参数**：\n",
    "   - 相比于VGG等网络，GoogLeNet具有较少的参数，避免了过多的计算负担，同时仍然保持较高的准确率。\n",
    "\n",
    "3. **深度与多尺度特征学习**：\n",
    "   - 通过Inception模块，GoogLeNet能够同时从多个尺度提取特征，使得模型更具表达能力。\n",
    "\n",
    "4. **全局平均池化**：\n",
    "   - 使用全局平均池化代替全连接层，有效减少了模型参数数量，并提高了计算效率。\n",
    "\n",
    "---\n",
    "\n",
    "## **GoogLeNet的劣势**\n",
    "\n",
    "1. **网络结构较为复杂**：\n",
    "   - GoogLeNet的Inception模块和深度结构较为复杂，可能使得网络在训练时的调优和优化较为困难。\n",
    "\n",
    "2. **可解释性较差**：\n",
    "   - 由于Inception模块和深度结构的复杂性，GoogLeNet的可解释性较差，难以直观理解每个模块的作用。\n",
    "\n",
    "---\n",
    "\n",
    "## **GoogLeNet的架构**\n",
    "\n",
    "GoogLeNet的架构可以简化为以下几个关键部分：\n",
    "\n",
    "- **输入**：\n",
    "  - 输入大小为224x224x3的图像。\n",
    "  \n",
    "- **Inception模块**：\n",
    "  - 网络的核心由多个Inception模块堆叠组成，每个模块包含多个并行的卷积操作和池化操作。\n",
    "  \n",
    "- **全局平均池化**：\n",
    "  - 在最后一层使用全局平均池化来替代全连接层，从而减少参数数量。\n",
    "\n",
    "- **输出**：\n",
    "  - 输出层为一个softmax分类器，用于进行多类分类。\n",
    "\n",
    "---\n",
    "\n",
    "## **GoogLeNet的架构图**：\n",
    "\n",
    "### **Inception Module:**\n",
    "<img src=\"img/Inception_Module.png\">\n",
    "\n",
    "### **Whole Structure:**\n",
    "<img src=\"img/Whole_Structure.png\">\n",
    "\n",
    "---\n",
    "\n",
    "## **GoogLeNet架构总结**\n",
    "\n",
    "GoogLeNet通过Inception模块、高效的1x1卷积以及全局平均池化设计，成功地提高了计算效率和减少了参数数量，具有较强的多尺度特征提取能力，但在实现上相对较为复杂。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet的代码实现\n",
    "\n",
    "### 1. 必要库的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取当前文件的父目录\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname('__file__'),'..'))\n",
    "# 如果父目录不在 sys.path中，则添加\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "import utils\n",
    "    \n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                      else 'mps' if torch.mps.is_available()\n",
    "                      else 'cpu')\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.导入数据库并转换为DataLoader格式\n",
    "\n",
    "在这一部分中，我们使用torchvision.transforms 仿照原论文对图像进行预处理：\n",
    "- `Resize` 和 `CenterCrop`：调整图像大小以匹配模型输入。\n",
    "- `ToTensor`：将图像转换为张量。\n",
    "- `Normalize`：对像素值进行归一化，有助于模型更快收敛。\n",
    "- 在训练集中用 `RandomHorizontalFlip`使得图像随机水平翻转，增强图像\n",
    "\n",
    "⚠️ **初学者提示**：\n",
    "- 确保 `mean` 和 `std` 值与预训练模型使用的值一致。\n",
    "- 输入图像的大小应与模型期望的输入大小匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GoogLeNet原论文的预处理流程\n",
    "transform = {'train':transforms.Compose([\n",
    "    transforms.Resize(256),                    # 缩放图像，使得最短边为256像素\n",
    "    transforms.RandomCrop(224),                # 中心裁剪224x224\n",
    "    transforms.RandomHorizontalFlip(),         # 随机水平翻转\n",
    "    transforms.ToTensor(),                     # 转换为Tensor，并缩放到[0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # 按ImageNet均值和标准差归一化\n",
    "                         std=[0.229, 0.224, 0.225])]),\n",
    "             'val':transforms.Compose([\n",
    "    transforms.Resize(256),                    # 缩放图像，使得最短边为256像素\n",
    "    transforms.CenterCrop(224),                # 中心裁剪224x224\n",
    "    transforms.ToTensor(),                     # 转换为Tensor，并缩放到[0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # 按ImageNet均值和标准差归一化\n",
    "                         std=[0.229, 0.224, 0.225])])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n03444034\n"
     ]
    }
   ],
   "source": [
    "# 读取验证集的标签\n",
    "val_labels = {}\n",
    "with open('../Datasets/tiny-imagenet-200/val/val_annotations.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        filename, label = parts[0], parts[1]\n",
    "        val_labels[filename] = label\n",
    "print(val_labels['val_0.JPEG'])\n",
    "\n",
    "# 首先将训练数据集加载到DataLoader中\n",
    "ImageNet_train = datasets.ImageFolder(root='../Datasets/tiny-imagenet-200/train',transform=transform['train'])\n",
    "ImageNet_train_iter = DataLoader(ImageNet_train,batch_size=64,shuffle=True,num_workers=0)\n",
    "\n",
    "# 自定义验证集数据集\n",
    "class TinyImageNetValDataset(Dataset):\n",
    "    def __init__(self, img_dir, val_labels, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_dict = val_labels\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(list(set(self.img_dict.values())))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.img_list = list(self.img_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_list[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.class_to_idx[self.img_dict[img_name]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "ImageNet_val = TinyImageNetValDataset(\n",
    "    img_dir='../Datasets/tiny-imagenet-200/val/images',\n",
    "    val_labels=val_labels,\n",
    "    transform=transform['val'])\n",
    "ImageNet_val_iter = DataLoader(ImageNet_val, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n",
      "tensor([ 25,  55, 182,  47, 150,  86,  26, 145,  73,  62, 155, 110, 174, 194,\n",
      "        179, 103,  89,  94,  84, 121,  31,  91,  90,  71,  73,  66, 177,  31,\n",
      "        119, 155, 152,  98, 167,  37, 177, 138,  76, 136,  60,  52,  31,  81,\n",
      "          8, 148, 126,  37, 139, 145, 161, 114,  20, 179, 130, 184,  82, 121,\n",
      "         71, 190,  87, 190,  52,  93, 196,  79])\n",
      "2\n",
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n",
      "tensor([107, 139, 140,  69,  69, 161, 147,  73, 145,  39, 158, 188,  39, 120,\n",
      "         21,  79,  33, 165, 153, 119, 123, 124,  18,   1, 170, 141, 171, 158,\n",
      "         79,  25, 121, 179, 124, 130, 176,  86,   6, 129, 142, 127,  33,  33,\n",
      "         44,  40, 110,  40, 159, 184,  57,  80,   9,  34,  39,  14,  75, 199,\n",
      "         10,  99,  66, 160,  69, 177,  25, 101])\n"
     ]
    }
   ],
   "source": [
    "# 尝试输出两个DataLoader中的第一批张量的形状\n",
    "img_train = next(iter(ImageNet_train_iter))\n",
    "print(len(img_train))\n",
    "print(img_train[0].shape)\n",
    "print(img_train[1].shape)\n",
    "print(img_train[1])\n",
    "\n",
    "img_val = next(iter(ImageNet_val_iter))\n",
    "print(len(img_val))\n",
    "print(img_val[0].shape)\n",
    "print(img_val[1].shape)\n",
    "print(img_val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. **Inception Module的构建规律**\n",
    "\n",
    "Inception模块是GoogLeNet的核心创新之一，它通过并行计算不同尺寸的卷积和池化操作，从而捕捉不同尺度的特征。Inception模块的主要思想是通过组合不同类型的卷积和池化操作，来增强网络的表达能力，并在保持计算效率的同时提高性能。\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. 核心设计理念**\n",
    "- **多尺度特征提取：** 在同一层中并行使用不同大小的卷积核和池化操作，来捕捉不同尺度的图像特征。\n",
    "- **1x1卷积：** 使用1x1卷积来减少计算量和参数数量，同时进行维度压缩和特征融合。\n",
    "- **并行结构：** 将不同大小的卷积操作（如1x1, 3x3, 5x5）和池化操作（如最大池化和平均池化）并行组合，获取不同尺度的特征。\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. Inception模块的具体构成**\n",
    "\n",
    "Inception模块一般包括以下几种操作，每种操作都具有不同的卷积核大小或池化方式：\n",
    "\n",
    "- **1x1卷积：** 用于减少输入特征图的深度（即通道数），从而减少计算量。\n",
    "- **3x3卷积：** 用来捕捉较小尺度的特征，通常与1x1卷积结合使用以降低计算复杂度。\n",
    "- **5x5卷积：** 用来捕捉更大尺度的特征，通常也会先使用1x1卷积来减少通道数。\n",
    "- **最大池化（Max Pooling）：** 用于提取局部特征，通常与1x1卷积结合进行特征降维。\n",
    "- **平均池化（Average Pooling）：** 用于降维和捕捉图像的全局信息。\n",
    "\n",
    "---\n",
    "\n",
    "#### **3. Inception模块的具体步骤**\n",
    "\n",
    "1. **输入：** 输入特征图尺寸为 \\( W \\times H \\times D \\)（宽度 x 高度 x 深度）。\n",
    "2. **1x1卷积：** $Y_{1} = Conv1\\times1(X)$\n",
    "3. **3x3卷积：** $Y_{2} = Conv3\\times3(Conv1\\times1(X))$\n",
    "4. **5x5卷积：** $Y_{3} = Conv5\\times5(Conv1\\times1(X))$\n",
    "5. **最大池化：** $Y_{4} = MaxPool(Conv1\\times1(X)$\n",
    "6. **输出拼接：** $Y = [Y_{1},Y_{2},Y_{3},Y_{4}]$\n",
    "\n",
    "---\n",
    "\n",
    "#### **4. Inception模块的优化**\n",
    "- **1x1卷积的使用：** 降低通道数，减少计算量。\n",
    "- **并行结构：** 同时考虑局部和全局的图像信息。\n",
    "- **减少计算量：** 显著减少计算量和参数数量。\n",
    "\n",
    "---\n",
    "\n",
    "#### **5. 典型的Inception模块示意图**\n",
    "\n",
    "Inception模块通常包含如下几个部分：\n",
    "\n",
    "- 1x1卷积\n",
    "- 1x1卷积 -> 3x3卷积\n",
    "- 1x1卷积 -> 5x5卷积\n",
    "- 3x3最大池化 -> 1x1卷积\n",
    "\n",
    "这些操作并行进行，然后将它们的输出在通道维度上进行拼接。\n",
    "\n",
    "**再放一遍Inception_Module的图，加深印象**\n",
    "<img src=\"img/Inception_Module.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 352, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self,in_channels,out_1,mid_2,out_2,mid_3,out_3,out_4):\n",
    "        super().__init__()\n",
    "               \n",
    "        # 1x1卷积层的定义\n",
    "        self.Y1 = nn.Conv2d(in_channels,out_1,kernel_size=1)\n",
    "        \n",
    "        # 1x1 -> 3x3卷积层的定义\n",
    "        self.Y2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,mid_2,kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(mid_2,out_2,kernel_size=3,padding=1),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        # 1x1 -> 5x5卷积层的定义\n",
    "        self.Y3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels,mid_3,kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(mid_3,out_3,kernel_size=5,padding=2),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        # 3x3 MaxPool -> 1x1 卷积层的定义\n",
    "        self.Y4 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels,out_4,kernel_size=1),\n",
    "            nn.ReLU())\n",
    "        \n",
    "    # 利用前向函数forward来拼接四个输出\n",
    "    def forward(self,x):\n",
    "        branch_1 = self.Y1(x)\n",
    "        branch_2 = self.Y2(x)\n",
    "        branch_3 = self.Y3(x)\n",
    "        branch_4 = self.Y4(x)\n",
    "        out = torch.cat([branch_1,branch_2,branch_3,branch_4],dim=1)\n",
    "        return out\n",
    "\n",
    "# 检验一下输出形状是否正确\n",
    "model = InceptionModule(3,64,64,128,32,128,32)\n",
    "input_tensor = torch.randn(1, 3, 224, 224)\n",
    "output_tensor = model(input_tensor)\n",
    "print(output_tensor.shape)\n",
    "# 输出的总的通道数应该是上面四个卷积层的out_channels相加 64+128+128+32=352"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 搭建GoogLeNet神经网络模型\n",
    "实际上我们完全可以定义一个比较通用的GoogLeNet模型，通过传入参数设置网络的深度和广度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GoogLeNet(\n",
      "  (pre_layers): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (4): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (Google_1): Sequential(\n",
      "    (0): InceptionModule(\n",
      "      (Y1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (Y2): Sequential(\n",
      "        (0): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y3): Sequential(\n",
      "        (0): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): InceptionModule(\n",
      "      (Y1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (Y2): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y3): Sequential(\n",
      "        (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (Google_2): Sequential(\n",
      "    (0): InceptionModule(\n",
      "      (Y1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (Y2): Sequential(\n",
      "        (0): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y3): Sequential(\n",
      "        (0): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): InceptionModule(\n",
      "      (Y1): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (Y2): Sequential(\n",
      "        (0): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y3): Sequential(\n",
      "        (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (2): InceptionModule(\n",
      "      (Y1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (Y2): Sequential(\n",
      "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y3): Sequential(\n",
      "        (0): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (3): InceptionModule(\n",
      "      (Y1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (Y2): Sequential(\n",
      "        (0): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y3): Sequential(\n",
      "        (0): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (4): InceptionModule(\n",
      "      (Y1): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (Y2): Sequential(\n",
      "        (0): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y3): Sequential(\n",
      "        (0): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (Google_3): Sequential(\n",
      "    (0): InceptionModule(\n",
      "      (Y1): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (Y2): Sequential(\n",
      "        (0): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y3): Sequential(\n",
      "        (0): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (1): InceptionModule(\n",
      "      (Y1): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (Y2): Sequential(\n",
      "        (0): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y3): Sequential(\n",
      "        (0): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (Y4): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): ReLU()\n",
      "        (2): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        (3): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (classifier_1): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Dropout(p=0.4, inplace=False)\n",
      "  )\n",
      "  (classifier_2): Linear(in_features=1024, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义完整的GoogLeNet架构\n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self,num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2, ceil_mode=True),\n",
    "            # 你会发现这实际上是两个小的VGG块，在此我们不做单独定义\n",
    "            nn.Conv2d(64, 64, kernel_size=1),\n",
    "            nn.Conv2d(64, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2, ceil_mode=True))\n",
    "\n",
    "        self.Google_1 = nn.Sequential(\n",
    "            InceptionModule(192, 64, 96, 128, 16, 32, 32), # 输出通道数：64+128+32+32=256\n",
    "            InceptionModule(256, 128, 128, 192, 32, 96, 64), # 输出通道数：128+192+96+64=480\n",
    "            nn.MaxPool2d(3, stride=2, ceil_mode=True))\n",
    "\n",
    "        self.Google_2 = nn.Sequential(\n",
    "            InceptionModule(480, 192, 96, 208, 16, 48, 64), # 输出通道数：192+208+48+64=512\n",
    "            InceptionModule(512, 160, 112, 224, 24, 64, 64),\n",
    "            InceptionModule(512, 128, 128, 256, 24, 64, 64),\n",
    "            InceptionModule(512, 112, 144, 288, 32, 64, 64),\n",
    "            InceptionModule(528, 256, 160, 320, 32, 128, 128),\n",
    "            nn.MaxPool2d(3, stride=2, ceil_mode=True))\n",
    "\n",
    "        self.Google_3 = nn.Sequential(\n",
    "            InceptionModule(832, 256, 160, 320, 32, 128, 128),\n",
    "            InceptionModule(832, 384, 192, 384, 48, 128, 128))\n",
    "\n",
    "        self.classifier_1 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), # 将7*7特征图池化为单一向量\n",
    "            nn.Dropout(0.4)) # 防止过拟合\n",
    "        \n",
    "        self.classifier_2 = nn.Linear(1024, num_classes) # 最终分类\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layers(x)\n",
    "        x = self.Google_1(x)\n",
    "        x = self.Google_2(x)\n",
    "        x = self.Google_3(x)\n",
    "        x = self.classifier_1(x)\n",
    "        x = torch.flatten(x,1)\n",
    "        x = self.classifier_2(x)\n",
    "        return x\n",
    "\n",
    "net = GoogLeNet(200)\n",
    "# 看一下壮观的GoogLeNet~\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉有些迷糊？没关系，用torchinfo中的summary模块可以看到一个固定形状的tensor经过每一个模块之后的形状变化！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,472\n",
      "              ReLU-2         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-3           [-1, 64, 56, 56]               0\n",
      "            Conv2d-4           [-1, 64, 56, 56]           4,160\n",
      "            Conv2d-5          [-1, 192, 56, 56]         110,784\n",
      "              ReLU-6          [-1, 192, 56, 56]               0\n",
      "         MaxPool2d-7          [-1, 192, 28, 28]               0\n",
      "            Conv2d-8           [-1, 64, 28, 28]          12,352\n",
      "            Conv2d-9           [-1, 96, 28, 28]          18,528\n",
      "             ReLU-10           [-1, 96, 28, 28]               0\n",
      "           Conv2d-11          [-1, 128, 28, 28]         110,720\n",
      "             ReLU-12          [-1, 128, 28, 28]               0\n",
      "           Conv2d-13           [-1, 16, 28, 28]           3,088\n",
      "             ReLU-14           [-1, 16, 28, 28]               0\n",
      "           Conv2d-15           [-1, 32, 28, 28]          12,832\n",
      "             ReLU-16           [-1, 32, 28, 28]               0\n",
      "        MaxPool2d-17          [-1, 192, 28, 28]               0\n",
      "             ReLU-18          [-1, 192, 28, 28]               0\n",
      "           Conv2d-19           [-1, 32, 28, 28]           6,176\n",
      "             ReLU-20           [-1, 32, 28, 28]               0\n",
      "  InceptionModule-21          [-1, 256, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]          32,896\n",
      "           Conv2d-23          [-1, 128, 28, 28]          32,896\n",
      "             ReLU-24          [-1, 128, 28, 28]               0\n",
      "           Conv2d-25          [-1, 192, 28, 28]         221,376\n",
      "             ReLU-26          [-1, 192, 28, 28]               0\n",
      "           Conv2d-27           [-1, 32, 28, 28]           8,224\n",
      "             ReLU-28           [-1, 32, 28, 28]               0\n",
      "           Conv2d-29           [-1, 96, 28, 28]          76,896\n",
      "             ReLU-30           [-1, 96, 28, 28]               0\n",
      "        MaxPool2d-31          [-1, 256, 28, 28]               0\n",
      "             ReLU-32          [-1, 256, 28, 28]               0\n",
      "           Conv2d-33           [-1, 64, 28, 28]          16,448\n",
      "             ReLU-34           [-1, 64, 28, 28]               0\n",
      "  InceptionModule-35          [-1, 480, 28, 28]               0\n",
      "        MaxPool2d-36          [-1, 480, 14, 14]               0\n",
      "           Conv2d-37          [-1, 192, 14, 14]          92,352\n",
      "           Conv2d-38           [-1, 96, 14, 14]          46,176\n",
      "             ReLU-39           [-1, 96, 14, 14]               0\n",
      "           Conv2d-40          [-1, 208, 14, 14]         179,920\n",
      "             ReLU-41          [-1, 208, 14, 14]               0\n",
      "           Conv2d-42           [-1, 16, 14, 14]           7,696\n",
      "             ReLU-43           [-1, 16, 14, 14]               0\n",
      "           Conv2d-44           [-1, 48, 14, 14]          19,248\n",
      "             ReLU-45           [-1, 48, 14, 14]               0\n",
      "        MaxPool2d-46          [-1, 480, 14, 14]               0\n",
      "             ReLU-47          [-1, 480, 14, 14]               0\n",
      "           Conv2d-48           [-1, 64, 14, 14]          30,784\n",
      "             ReLU-49           [-1, 64, 14, 14]               0\n",
      "  InceptionModule-50          [-1, 512, 14, 14]               0\n",
      "           Conv2d-51          [-1, 160, 14, 14]          82,080\n",
      "           Conv2d-52          [-1, 112, 14, 14]          57,456\n",
      "             ReLU-53          [-1, 112, 14, 14]               0\n",
      "           Conv2d-54          [-1, 224, 14, 14]         226,016\n",
      "             ReLU-55          [-1, 224, 14, 14]               0\n",
      "           Conv2d-56           [-1, 24, 14, 14]          12,312\n",
      "             ReLU-57           [-1, 24, 14, 14]               0\n",
      "           Conv2d-58           [-1, 64, 14, 14]          38,464\n",
      "             ReLU-59           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-60          [-1, 512, 14, 14]               0\n",
      "             ReLU-61          [-1, 512, 14, 14]               0\n",
      "           Conv2d-62           [-1, 64, 14, 14]          32,832\n",
      "             ReLU-63           [-1, 64, 14, 14]               0\n",
      "  InceptionModule-64          [-1, 512, 14, 14]               0\n",
      "           Conv2d-65          [-1, 128, 14, 14]          65,664\n",
      "           Conv2d-66          [-1, 128, 14, 14]          65,664\n",
      "             ReLU-67          [-1, 128, 14, 14]               0\n",
      "           Conv2d-68          [-1, 256, 14, 14]         295,168\n",
      "             ReLU-69          [-1, 256, 14, 14]               0\n",
      "           Conv2d-70           [-1, 24, 14, 14]          12,312\n",
      "             ReLU-71           [-1, 24, 14, 14]               0\n",
      "           Conv2d-72           [-1, 64, 14, 14]          38,464\n",
      "             ReLU-73           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-74          [-1, 512, 14, 14]               0\n",
      "             ReLU-75          [-1, 512, 14, 14]               0\n",
      "           Conv2d-76           [-1, 64, 14, 14]          32,832\n",
      "             ReLU-77           [-1, 64, 14, 14]               0\n",
      "  InceptionModule-78          [-1, 512, 14, 14]               0\n",
      "           Conv2d-79          [-1, 112, 14, 14]          57,456\n",
      "           Conv2d-80          [-1, 144, 14, 14]          73,872\n",
      "             ReLU-81          [-1, 144, 14, 14]               0\n",
      "           Conv2d-82          [-1, 288, 14, 14]         373,536\n",
      "             ReLU-83          [-1, 288, 14, 14]               0\n",
      "           Conv2d-84           [-1, 32, 14, 14]          16,416\n",
      "             ReLU-85           [-1, 32, 14, 14]               0\n",
      "           Conv2d-86           [-1, 64, 14, 14]          51,264\n",
      "             ReLU-87           [-1, 64, 14, 14]               0\n",
      "        MaxPool2d-88          [-1, 512, 14, 14]               0\n",
      "             ReLU-89          [-1, 512, 14, 14]               0\n",
      "           Conv2d-90           [-1, 64, 14, 14]          32,832\n",
      "             ReLU-91           [-1, 64, 14, 14]               0\n",
      "  InceptionModule-92          [-1, 528, 14, 14]               0\n",
      "           Conv2d-93          [-1, 256, 14, 14]         135,424\n",
      "           Conv2d-94          [-1, 160, 14, 14]          84,640\n",
      "             ReLU-95          [-1, 160, 14, 14]               0\n",
      "           Conv2d-96          [-1, 320, 14, 14]         461,120\n",
      "             ReLU-97          [-1, 320, 14, 14]               0\n",
      "           Conv2d-98           [-1, 32, 14, 14]          16,928\n",
      "             ReLU-99           [-1, 32, 14, 14]               0\n",
      "          Conv2d-100          [-1, 128, 14, 14]         102,528\n",
      "            ReLU-101          [-1, 128, 14, 14]               0\n",
      "       MaxPool2d-102          [-1, 528, 14, 14]               0\n",
      "            ReLU-103          [-1, 528, 14, 14]               0\n",
      "          Conv2d-104          [-1, 128, 14, 14]          67,712\n",
      "            ReLU-105          [-1, 128, 14, 14]               0\n",
      " InceptionModule-106          [-1, 832, 14, 14]               0\n",
      "       MaxPool2d-107            [-1, 832, 7, 7]               0\n",
      "          Conv2d-108            [-1, 256, 7, 7]         213,248\n",
      "          Conv2d-109            [-1, 160, 7, 7]         133,280\n",
      "            ReLU-110            [-1, 160, 7, 7]               0\n",
      "          Conv2d-111            [-1, 320, 7, 7]         461,120\n",
      "            ReLU-112            [-1, 320, 7, 7]               0\n",
      "          Conv2d-113             [-1, 32, 7, 7]          26,656\n",
      "            ReLU-114             [-1, 32, 7, 7]               0\n",
      "          Conv2d-115            [-1, 128, 7, 7]         102,528\n",
      "            ReLU-116            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-117            [-1, 832, 7, 7]               0\n",
      "            ReLU-118            [-1, 832, 7, 7]               0\n",
      "          Conv2d-119            [-1, 128, 7, 7]         106,624\n",
      "            ReLU-120            [-1, 128, 7, 7]               0\n",
      " InceptionModule-121            [-1, 832, 7, 7]               0\n",
      "          Conv2d-122            [-1, 384, 7, 7]         319,872\n",
      "          Conv2d-123            [-1, 192, 7, 7]         159,936\n",
      "            ReLU-124            [-1, 192, 7, 7]               0\n",
      "          Conv2d-125            [-1, 384, 7, 7]         663,936\n",
      "            ReLU-126            [-1, 384, 7, 7]               0\n",
      "          Conv2d-127             [-1, 48, 7, 7]          39,984\n",
      "            ReLU-128             [-1, 48, 7, 7]               0\n",
      "          Conv2d-129            [-1, 128, 7, 7]         153,728\n",
      "            ReLU-130            [-1, 128, 7, 7]               0\n",
      "       MaxPool2d-131            [-1, 832, 7, 7]               0\n",
      "            ReLU-132            [-1, 832, 7, 7]               0\n",
      "          Conv2d-133            [-1, 128, 7, 7]         106,624\n",
      "            ReLU-134            [-1, 128, 7, 7]               0\n",
      " InceptionModule-135           [-1, 1024, 7, 7]               0\n",
      "AdaptiveAvgPool2d-136           [-1, 1024, 1, 1]               0\n",
      "         Dropout-137           [-1, 1024, 1, 1]               0\n",
      "          Linear-138                  [-1, 200]         205,000\n",
      "================================================================\n",
      "Total params: 6,178,552\n",
      "Trainable params: 6,178,552\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 72.40\n",
      "Params size (MB): 23.57\n",
      "Estimated Total Size (MB): 96.55\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(net,input_size=(3,224,224),device=\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.trianing_loop(net,loss_fn,optimizer,ImageNet_train_iter,ImageNet_val_iter,15,device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
