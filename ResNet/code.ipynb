{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet（Residual Network）\n",
    "\n",
    "ResNet是由微软研究院的Kaiming He等人在2015年提出的深度卷积神经网络架构，它在ImageNet和COCO等计算机视觉任务中取得了显著的成绩。ResNet的核心思想是引入了**残差学习（Residual Learning）**，解决了深度神经网络中的**梯度消失**和**退化问题**，使得网络可以训练得更深、更高效。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. ResNet的特点\n",
    "\n",
    "- **残差块（Residual Block）**：ResNet的核心是残差块，它引入了“跳跃连接”（Skip Connection）或“短路连接”（Shortcut Connection）。残差块的输入直接与输出相加，形成一个残差映射 $F(x) = H(x) - x$，其中 $H(x)$ 是目标映射，$x$ 是输入。通过这种方式，网络可以学习残差 $F(x)$ 而不是直接学习 $H(x)$。\n",
    "<img src=\"img/Residual_Block.png\">\n",
    "- **深度网络**：ResNet可以构建非常深的网络（如ResNet-152），而不会出现梯度消失或退化问题。\n",
    "- **恒等映射**：跳跃连接允许网络在某些层中直接传递输入，从而简化了训练过程。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ResNet的优势\n",
    "\n",
    "- **解决梯度消失问题**：跳跃连接使得梯度可以直接传递到前面的层，缓解了深度网络中的梯度消失问题。\n",
    "- **解决退化问题**：即使网络很深，ResNet的性能也不会随着层数的增加而显著下降。\n",
    "- **高效训练**：残差学习使得网络更容易优化，训练速度更快。\n",
    "- **可扩展性**：ResNet可以扩展到非常深的网络（如ResNet-101、ResNet-152等），同时保持高性能。\n",
    "- **通用性强**：ResNet不仅在图像分类任务中表现优异，还在目标检测、语义分割等任务中取得了显著效果。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ResNet的劣势\n",
    "\n",
    "- **计算成本高**：虽然ResNet解决了深度网络的训练问题，但非常深的网络（如ResNet-152）仍然需要大量的计算资源。\n",
    "- **参数较多**：深层的ResNet模型参数较多，可能在某些资源受限的场景中不适用。\n",
    "- **过拟合风险**：对于较小的数据集，过深的ResNet可能会导致过拟合。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ResNet的架构\n",
    "\n",
    "ResNet的基本单元是**残差块**，其结构如下：\n",
    "\n",
    "- **普通残差块**：由两个3x3卷积层组成，输入通过跳跃连接直接加到输出上。\n",
    "- **瓶颈残差块**：为了减少计算量，ResNet引入了瓶颈结构，由1x1、3x3、1x1卷积层组成，其中1x1卷积层用于降维和升维。\n",
    "\n",
    "ResNet的主要版本包括：\n",
    "\n",
    "- **ResNet-18/34**：使用普通残差块。\n",
    "- **ResNet-50/101/152**：使用瓶颈残差块。\n",
    "\n",
    "### 残差块的数学表达\n",
    "\n",
    "对于输入 $x$，残差块的输出为：\n",
    "$$\n",
    "y = F(x, W_{i}) + x\n",
    "$$\n",
    "其中 $F(x, W_{i})$ 是残差映射，$x$ 是跳跃连接的输入。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ResNet的应用\n",
    "\n",
    "ResNet广泛应用于各种计算机视觉任务，包括：\n",
    "\n",
    "- **图像分类**：在ImageNet等数据集上取得了优异的分类性能。\n",
    "- **目标检测**：如Faster R-CNN、Mask R-CNN等检测框架中使用了ResNet作为骨干网络。\n",
    "- **语义分割**：ResNet在分割任务中也被广泛使用。\n",
    "- **迁移学习**：ResNet的预训练模型常用于其他任务的迁移学习。\n",
    "\n",
    "---\n",
    "\n",
    "## 6. ResNet与其他网络的对比示意图\n",
    "\n",
    "<img src=\"img/Comparison.png\">\n",
    "\n",
    "---\n",
    "\n",
    "## 总结\n",
    "\n",
    "ResNet通过引入残差学习和跳跃连接，解决了深度神经网络中的梯度消失和退化问题，使得网络可以训练得更深、更高效。虽然它在计算成本和参数量上有一定劣势，但其在计算机视觉任务中的表现和通用性使其成为深度学习领域的重要里程碑。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet 的代码实现\n",
    "\n",
    "### 1. 必要库的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torch import functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取当前文件的父目录\n",
    "parent_dir = os.path.abspath(os.path.join(os.path.dirname('__file__'),'..'))\n",
    "# 如果父目录不在 sys.path中，则添加\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.append(parent_dir)\n",
    "import utils\n",
    "    \n",
    "%matplotlib inline\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available()\n",
    "                      else 'mps' if torch.mps.is_available()\n",
    "                      else 'cpu')\n",
    "print(torch.__version__)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.导入数据库并转换为DataLoader格式\n",
    "\n",
    "在这一部分中，我们使用torchvision.transforms 仿照原论文对图像进行预处理：\n",
    "- `Resize` 和 `CenterCrop`：调整图像大小以匹配模型输入。\n",
    "- `ToTensor`：将图像转换为张量。\n",
    "- `Normalize`：对像素值进行归一化，有助于模型更快收敛。\n",
    "- 在训练集中用 `RandomHorizontalFlip`使得图像随机水平翻转，增强图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于ImageNet数据集，我们使用ImageNet的均值和标准差来归一化图像。\n",
    "transform = {'train':transforms.Compose([\n",
    "    transforms.Resize(256),                    # 缩放图像，使得最短边为256像素\n",
    "    transforms.RandomCrop(224),                # 中心裁剪224x224\n",
    "    transforms.RandomHorizontalFlip(),         # 随机水平翻转\n",
    "    transforms.ToTensor(),                     # 转换为Tensor，并缩放到[0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # 按ImageNet均值和标准差归一化\n",
    "                         std=[0.229, 0.224, 0.225])]),\n",
    "             'val':transforms.Compose([\n",
    "    transforms.Resize(256),                    # 缩放图像，使得最短边为256像素\n",
    "    transforms.CenterCrop(224),                # 中心裁剪224x224\n",
    "    transforms.ToTensor(),                     # 转换为Tensor，并缩放到[0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # 按ImageNet均值和标准差归一化\n",
    "                         std=[0.229, 0.224, 0.225])])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n03444034\n",
      "2\n",
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n",
      "tensor([199,  58,  22,  29, 142,  48, 131, 147, 145,  68, 126, 182, 124,  12,\n",
      "         50,  95,  73,  92, 106,   9,   9, 166,  52,  16,  87,  14,  31, 121,\n",
      "        128,  49,  51, 157,  12, 191, 185,  98,  18,  62,  96,  38, 156,  42,\n",
      "        171,  82,  21, 159,  61,  34,  69,  47, 113, 197, 175,  22, 191, 162,\n",
      "         38, 152,  50,  28,  63, 144, 180, 164])\n",
      "2\n",
      "torch.Size([64, 3, 224, 224])\n",
      "torch.Size([64])\n",
      "tensor([107, 139, 140,  69,  69, 161, 147,  73, 145,  39, 158, 188,  39, 120,\n",
      "         21,  79,  33, 165, 153, 119, 123, 124,  18,   1, 170, 141, 171, 158,\n",
      "         79,  25, 121, 179, 124, 130, 176,  86,   6, 129, 142, 127,  33,  33,\n",
      "         44,  40, 110,  40, 159, 184,  57,  80,   9,  34,  39,  14,  75, 199,\n",
      "         10,  99,  66, 160,  69, 177,  25, 101])\n"
     ]
    }
   ],
   "source": [
    "# 读取验证集的标签\n",
    "val_labels = {}\n",
    "with open('../Datasets/tiny-imagenet-200/val/val_annotations.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        filename, label = parts[0], parts[1]\n",
    "        val_labels[filename] = label\n",
    "print(val_labels['val_0.JPEG'])\n",
    "\n",
    "# 首先将训练数据集加载到DataLoader中\n",
    "ImageNet_train = datasets.ImageFolder(root='../Datasets/tiny-imagenet-200/train',transform=transform['train'])\n",
    "ImageNet_train_iter = DataLoader(ImageNet_train,batch_size=64,shuffle=True,num_workers=0)\n",
    "\n",
    "# 自定义验证集数据集\n",
    "class TinyImageNetValDataset(Dataset):\n",
    "    def __init__(self, img_dir, val_labels, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.img_dict = val_labels\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(list(set(self.img_dict.values())))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.img_list = list(self.img_dict.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_list[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.class_to_idx[self.img_dict[img_name]]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "ImageNet_val = TinyImageNetValDataset(\n",
    "    img_dir='../Datasets/tiny-imagenet-200/val/images',\n",
    "    val_labels=val_labels,\n",
    "    transform=transform['val'])\n",
    "ImageNet_val_iter = DataLoader(ImageNet_val, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "# 尝试输出两个DataLoader中的第一批张量的形状\n",
    "img_train = next(iter(ImageNet_train_iter))\n",
    "print(len(img_train))\n",
    "print(img_train[0].shape)\n",
    "print(img_train[1].shape)\n",
    "print(img_train[1])\n",
    "\n",
    "img_val = next(iter(ImageNet_val_iter))\n",
    "print(len(img_val))\n",
    "print(img_val[0].shape)\n",
    "print(img_val[1].shape)\n",
    "print(img_val[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 尝试构建一个最基本的ResNet块\n",
    "ResNet显然同样是由多个相似的残差块构成的，我们先定义好一个最基本的残差块，即一个包含两个卷积层的残差块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        # 第一个卷积层\n",
    "        self.conv1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True))\n",
    "        \n",
    "        # 第二个卷积层\n",
    "        self.conv2 = nn.Sequential(\n",
    "        nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "        nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        # 跳跃连接\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 如果输入通道数和输出通道数不同，则需要添加卷积层和BN层以改变x的形状（即通道数和像素数）\n",
    "        if stride != 1 or in_channels != out_channels: \n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 残差映射\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        # 跳跃连接\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**卷积层：** 使用两个3x3卷积层，每个卷积层后接批量归一化（BatchNorm）和ReLU激活函数。\n",
    "\n",
    "**跳跃连接：** 如果输入和输出的通道数或尺寸不一致（例如通过步幅 stride 调整大小），则使用1x1卷积调整输入的通道数和尺寸，以便与输出相加。\n",
    "\n",
    "**残差映射：** 对于每个残差块，计算残差映射 $F(x)$，然后将输入 $x$ 通过跳跃连接加到输出上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 开始用ResNet的基本块来搭建整个ResNet网络\n",
    "\n",
    "接下来，我们可以通过堆叠多个BasicBlock来构建ResNet模型。ResNet模型通常由多个“阶段”（stage）组成，每个阶段包含多个ResNet块，且每个阶段的特征图尺寸会逐渐减小，通道数会逐渐增加。\n",
    "\n",
    "以下是一个简单的ResNet-18模型的实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 200]         102,600\n",
      "================================================================\n",
      "Total params: 11,279,112\n",
      "Trainable params: 11,279,112\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 62.79\n",
      "Params size (MB): 43.03\n",
      "Estimated Total Size (MB): 106.39\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sympy import Basic\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_channels = 64 # 在后面的_make_layer函数中会用到\n",
    "        \n",
    "        self.pre_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "            )\n",
    "        self.main_layers = nn.Sequential(\n",
    "            self._make_layer(block, 64, num_blocks[0], stride=1),\n",
    "            self._make_layer(block, 128, num_blocks[1], stride=2),\n",
    "            self._make_layer(block, 256, num_blocks[2], stride=2),\n",
    "            self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "            )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self,block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pre_layers(x)\n",
    "        x = self.main_layers(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def resnet18(num_classes=200):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes)\n",
    "\n",
    "model = resnet18()\n",
    "summary(model,(3,224,224),device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解释\n",
    "`_make_layer`函数用于创建一个由多个ResNet块组成的层。每个层的第一个块的步幅（stride）可以大于1，用于下采样特征图。\n",
    "\n",
    "`num_blocks`列表指定了每个层中ResNet块的数量。例如，[2, 2, 2, 2]表示每个层有2个ResNet块，这是ResNet-18的结构。\n",
    "\n",
    "`pre_layers`是ResNet的预处理层，包括卷积层、BN层、ReLU激活函数层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （选读）ResNet基本块的变体：Bottleneck ResNet Block\n",
    "\n",
    "在ResNet-50、ResNet-101和ResNet-152中，作者发现这种基本块的效率较低，因此提出了一种新的Bottleneck ResNet Block\n",
    "\n",
    "####  Bottleneck Block 的结构\n",
    "Bottleneck Block 是 ResNet 中的一种基本构建单元，其核心思想是通过 降维-升维 的方式减少计算量。它的结构通常由三个卷积层组成，具体如下：\n",
    "\n",
    "1. **1x1 卷积层：**\n",
    "- 作用：降维，减少通道数。例如，将输入特征图的通道数从 256 减少到 64。\n",
    "- 目的：减少后续卷积层的计算量。\n",
    "\n",
    "2. **3x3 卷积层：**\n",
    "- 作用：提取空间特征。这是核心卷积层，用于捕捉图像的局部特征。\n",
    "- 目的：提取空间特征，提高模型的鲁棒性。\n",
    "\n",
    "3. **1x1 卷积层：**\n",
    "- 作用：升维，恢复通道数。例如，将通道数从 64 恢复到 256。\n",
    "- 目的：恢复特征图的维度，以便与输入特征图相加（残差连接）。\n",
    "\n",
    "#### 为什么叫 Bottleneck？\n",
    "Bottleneck Block 的名字来源于其结构设计：通过 1x1 卷积层先将特征图的通道数减少（“瓶颈”），然后再通过 3x3 卷积层提取特征，最后再通过 1x1 卷积层恢复通道数。这种设计类似于一个“瓶颈”，先压缩再扩展。\n",
    "\n",
    "#### 优点：\n",
    "- 减少计算量：3x3 卷积层的输入通道数减少，计算量大幅降低。\n",
    "- 减少参数数量：1x1 卷积层可以显著减少参数量。\n",
    "- 保持性能：通过残差连接，模型可以学习到更复杂的特征。\n",
    "\n",
    "#### Bottleneck Block 的数学表达\n",
    "假设输入特征图为 $x$，Bottleneck Block 的输出 $y$ 可以表示为：\n",
    "$$\n",
    "y = F(x) + x\n",
    "$$\n",
    "其中，$F(x)$ 是 Bottleneck Block 的核心操作，具体包括：\n",
    "\n",
    "- $F(x) = W_{2} \\cdot (W_{1} \\cdot x)$（1x1 卷积 + 3x3 卷积）\n",
    "\n",
    "- $F(x) = W_{3} \\cdot F(x)$ （1x1 卷积）\n",
    "\n",
    "其中，( $W_{1},W_{2},W_{3}$ ) 分别是三个卷积层的权重。\n",
    "\n",
    "#### Bottleneck Block 的应用\n",
    "Bottleneck Block 在 ResNet 及其变体（如 ResNet-50、ResNet-101、ResNet-152）中广泛使用。以下是 ResNet-50 中 Bottleneck Block 的具体参数示例：\n",
    "- 输入特征图：256 通道\n",
    "- 1x1 卷积层：64 通道\n",
    "- 3x3 卷积层：64 通道\n",
    "- 1x1 卷积层：256 通道\n",
    "\n",
    "通过这种设计，ResNet-50 可以在保持高性能的同时，显著减少计算量和参数量。\n",
    "\n",
    "#### 与普通 ResNet Block 的区别\n",
    "普通 ResNet Block（如 ResNet-18 中的 Basic Block）通常由两个 3x3 卷积层组成，而 Bottleneck Block 通过引入 1x1 卷积层进一步优化了计算效率。\n",
    "\n",
    "下面这张图中左边是普通 ResNet Block，右边是 Bottleneck Block。可以看到，Bottleneck Block 减少了计算量，同时保持了模型的性能。\n",
    "<img src=\"img/Bottleneck.png\">\n",
    "#### 代码实现\n",
    "\n",
    "下面是 ResNet-50 中的 Bottleneck Block 的代码实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Bottleneck Block\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels // 4, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.Conv2d(out_channels // 4, out_channels // 4, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels // 4),\n",
    "            nn.Conv2d(out_channels // 4, out_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True))\n",
    "        \n",
    "        # 跳跃连接\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 如果输入通道数和输出通道数不同，则需要添加卷积层和BN层以改变x的形状（即通道数和像素数）\n",
    "        if stride != 1 or in_channels != out_channels: \n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 残差映射\n",
    "        out = self.conv_block(x)\n",
    "        # 跳跃连接\n",
    "        out += self.shortcut(x)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 我们尝试用Bottleneck Block来实现一个比较深的ResNet: ResNet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 16, 56, 56]           1,024\n",
      "       BatchNorm2d-6           [-1, 16, 56, 56]              32\n",
      "            Conv2d-7           [-1, 16, 56, 56]           2,304\n",
      "       BatchNorm2d-8           [-1, 16, 56, 56]              32\n",
      "            Conv2d-9           [-1, 64, 56, 56]           1,024\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "             ReLU-11           [-1, 64, 56, 56]               0\n",
      "             ReLU-12           [-1, 64, 56, 56]               0\n",
      "  BottleneckBlock-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14           [-1, 16, 56, 56]           1,024\n",
      "      BatchNorm2d-15           [-1, 16, 56, 56]              32\n",
      "           Conv2d-16           [-1, 16, 56, 56]           2,304\n",
      "      BatchNorm2d-17           [-1, 16, 56, 56]              32\n",
      "           Conv2d-18           [-1, 64, 56, 56]           1,024\n",
      "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
      "             ReLU-20           [-1, 64, 56, 56]               0\n",
      "             ReLU-21           [-1, 64, 56, 56]               0\n",
      "  BottleneckBlock-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23           [-1, 16, 56, 56]           1,024\n",
      "      BatchNorm2d-24           [-1, 16, 56, 56]              32\n",
      "           Conv2d-25           [-1, 16, 56, 56]           2,304\n",
      "      BatchNorm2d-26           [-1, 16, 56, 56]              32\n",
      "           Conv2d-27           [-1, 64, 56, 56]           1,024\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "             ReLU-30           [-1, 64, 56, 56]               0\n",
      "  BottleneckBlock-31           [-1, 64, 56, 56]               0\n",
      "           Conv2d-32           [-1, 32, 56, 56]           2,048\n",
      "      BatchNorm2d-33           [-1, 32, 56, 56]              64\n",
      "           Conv2d-34           [-1, 32, 28, 28]           9,216\n",
      "      BatchNorm2d-35           [-1, 32, 28, 28]              64\n",
      "           Conv2d-36          [-1, 128, 28, 28]           4,096\n",
      "      BatchNorm2d-37          [-1, 128, 28, 28]             256\n",
      "             ReLU-38          [-1, 128, 28, 28]               0\n",
      "           Conv2d-39          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-40          [-1, 128, 28, 28]             256\n",
      "             ReLU-41          [-1, 128, 28, 28]               0\n",
      "  BottleneckBlock-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43           [-1, 32, 28, 28]           4,096\n",
      "      BatchNorm2d-44           [-1, 32, 28, 28]              64\n",
      "           Conv2d-45           [-1, 32, 28, 28]           9,216\n",
      "      BatchNorm2d-46           [-1, 32, 28, 28]              64\n",
      "           Conv2d-47          [-1, 128, 28, 28]           4,096\n",
      "      BatchNorm2d-48          [-1, 128, 28, 28]             256\n",
      "             ReLU-49          [-1, 128, 28, 28]               0\n",
      "             ReLU-50          [-1, 128, 28, 28]               0\n",
      "  BottleneckBlock-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52           [-1, 32, 28, 28]           4,096\n",
      "      BatchNorm2d-53           [-1, 32, 28, 28]              64\n",
      "           Conv2d-54           [-1, 32, 28, 28]           9,216\n",
      "      BatchNorm2d-55           [-1, 32, 28, 28]              64\n",
      "           Conv2d-56          [-1, 128, 28, 28]           4,096\n",
      "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
      "             ReLU-58          [-1, 128, 28, 28]               0\n",
      "             ReLU-59          [-1, 128, 28, 28]               0\n",
      "  BottleneckBlock-60          [-1, 128, 28, 28]               0\n",
      "           Conv2d-61           [-1, 32, 28, 28]           4,096\n",
      "      BatchNorm2d-62           [-1, 32, 28, 28]              64\n",
      "           Conv2d-63           [-1, 32, 28, 28]           9,216\n",
      "      BatchNorm2d-64           [-1, 32, 28, 28]              64\n",
      "           Conv2d-65          [-1, 128, 28, 28]           4,096\n",
      "      BatchNorm2d-66          [-1, 128, 28, 28]             256\n",
      "             ReLU-67          [-1, 128, 28, 28]               0\n",
      "             ReLU-68          [-1, 128, 28, 28]               0\n",
      "  BottleneckBlock-69          [-1, 128, 28, 28]               0\n",
      "           Conv2d-70           [-1, 64, 28, 28]           8,192\n",
      "      BatchNorm2d-71           [-1, 64, 28, 28]             128\n",
      "           Conv2d-72           [-1, 64, 14, 14]          36,864\n",
      "      BatchNorm2d-73           [-1, 64, 14, 14]             128\n",
      "           Conv2d-74          [-1, 256, 14, 14]          16,384\n",
      "      BatchNorm2d-75          [-1, 256, 14, 14]             512\n",
      "             ReLU-76          [-1, 256, 14, 14]               0\n",
      "           Conv2d-77          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-78          [-1, 256, 14, 14]             512\n",
      "             ReLU-79          [-1, 256, 14, 14]               0\n",
      "  BottleneckBlock-80          [-1, 256, 14, 14]               0\n",
      "           Conv2d-81           [-1, 64, 14, 14]          16,384\n",
      "      BatchNorm2d-82           [-1, 64, 14, 14]             128\n",
      "           Conv2d-83           [-1, 64, 14, 14]          36,864\n",
      "      BatchNorm2d-84           [-1, 64, 14, 14]             128\n",
      "           Conv2d-85          [-1, 256, 14, 14]          16,384\n",
      "      BatchNorm2d-86          [-1, 256, 14, 14]             512\n",
      "             ReLU-87          [-1, 256, 14, 14]               0\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "  BottleneckBlock-89          [-1, 256, 14, 14]               0\n",
      "           Conv2d-90           [-1, 64, 14, 14]          16,384\n",
      "      BatchNorm2d-91           [-1, 64, 14, 14]             128\n",
      "           Conv2d-92           [-1, 64, 14, 14]          36,864\n",
      "      BatchNorm2d-93           [-1, 64, 14, 14]             128\n",
      "           Conv2d-94          [-1, 256, 14, 14]          16,384\n",
      "      BatchNorm2d-95          [-1, 256, 14, 14]             512\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "             ReLU-97          [-1, 256, 14, 14]               0\n",
      "  BottleneckBlock-98          [-1, 256, 14, 14]               0\n",
      "           Conv2d-99           [-1, 64, 14, 14]          16,384\n",
      "     BatchNorm2d-100           [-1, 64, 14, 14]             128\n",
      "          Conv2d-101           [-1, 64, 14, 14]          36,864\n",
      "     BatchNorm2d-102           [-1, 64, 14, 14]             128\n",
      "          Conv2d-103          [-1, 256, 14, 14]          16,384\n",
      "     BatchNorm2d-104          [-1, 256, 14, 14]             512\n",
      "            ReLU-105          [-1, 256, 14, 14]               0\n",
      "            ReLU-106          [-1, 256, 14, 14]               0\n",
      " BottleneckBlock-107          [-1, 256, 14, 14]               0\n",
      "          Conv2d-108           [-1, 64, 14, 14]          16,384\n",
      "     BatchNorm2d-109           [-1, 64, 14, 14]             128\n",
      "          Conv2d-110           [-1, 64, 14, 14]          36,864\n",
      "     BatchNorm2d-111           [-1, 64, 14, 14]             128\n",
      "          Conv2d-112          [-1, 256, 14, 14]          16,384\n",
      "     BatchNorm2d-113          [-1, 256, 14, 14]             512\n",
      "            ReLU-114          [-1, 256, 14, 14]               0\n",
      "            ReLU-115          [-1, 256, 14, 14]               0\n",
      " BottleneckBlock-116          [-1, 256, 14, 14]               0\n",
      "          Conv2d-117           [-1, 64, 14, 14]          16,384\n",
      "     BatchNorm2d-118           [-1, 64, 14, 14]             128\n",
      "          Conv2d-119           [-1, 64, 14, 14]          36,864\n",
      "     BatchNorm2d-120           [-1, 64, 14, 14]             128\n",
      "          Conv2d-121          [-1, 256, 14, 14]          16,384\n",
      "     BatchNorm2d-122          [-1, 256, 14, 14]             512\n",
      "            ReLU-123          [-1, 256, 14, 14]               0\n",
      "            ReLU-124          [-1, 256, 14, 14]               0\n",
      " BottleneckBlock-125          [-1, 256, 14, 14]               0\n",
      "          Conv2d-126          [-1, 128, 14, 14]          32,768\n",
      "     BatchNorm2d-127          [-1, 128, 14, 14]             256\n",
      "          Conv2d-128            [-1, 128, 7, 7]         147,456\n",
      "     BatchNorm2d-129            [-1, 128, 7, 7]             256\n",
      "          Conv2d-130            [-1, 512, 7, 7]          65,536\n",
      "     BatchNorm2d-131            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-132            [-1, 512, 7, 7]               0\n",
      "          Conv2d-133            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-134            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-135            [-1, 512, 7, 7]               0\n",
      " BottleneckBlock-136            [-1, 512, 7, 7]               0\n",
      "          Conv2d-137            [-1, 128, 7, 7]          65,536\n",
      "     BatchNorm2d-138            [-1, 128, 7, 7]             256\n",
      "          Conv2d-139            [-1, 128, 7, 7]         147,456\n",
      "     BatchNorm2d-140            [-1, 128, 7, 7]             256\n",
      "          Conv2d-141            [-1, 512, 7, 7]          65,536\n",
      "     BatchNorm2d-142            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-143            [-1, 512, 7, 7]               0\n",
      "            ReLU-144            [-1, 512, 7, 7]               0\n",
      " BottleneckBlock-145            [-1, 512, 7, 7]               0\n",
      "          Conv2d-146            [-1, 128, 7, 7]          65,536\n",
      "     BatchNorm2d-147            [-1, 128, 7, 7]             256\n",
      "          Conv2d-148            [-1, 128, 7, 7]         147,456\n",
      "     BatchNorm2d-149            [-1, 128, 7, 7]             256\n",
      "          Conv2d-150            [-1, 512, 7, 7]          65,536\n",
      "     BatchNorm2d-151            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-152            [-1, 512, 7, 7]               0\n",
      "            ReLU-153            [-1, 512, 7, 7]               0\n",
      " BottleneckBlock-154            [-1, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-155            [-1, 512, 1, 1]               0\n",
      "          Linear-156                 [-1, 1000]         513,000\n",
      "================================================================\n",
      "Total params: 2,000,744\n",
      "Trainable params: 2,000,744\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 87.77\n",
      "Params size (MB): 7.63\n",
      "Estimated Total Size (MB): 95.98\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def resnet50(num_classes=1000):\n",
    "    return ResNet(BottleneckBlock, [3, 4, 6, 3], num_classes)\n",
    "\n",
    "model = resnet50()\n",
    "summary(model,(3,224,224),device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(),lr=0.001)\n",
    "# utils.trianing_loop(model,loss_fn,optimizer,ImageNet_train_iter,ImageNet_val_iter,15,device)\n",
    "# 自己跑去，我跑不动"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
